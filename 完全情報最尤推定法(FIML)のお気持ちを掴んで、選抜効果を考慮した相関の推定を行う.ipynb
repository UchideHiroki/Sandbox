{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# この記事は何"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この記事は、完全情報最尤推定法(FIML)を用いた簡単な仮説検証を行なった記事です。  \n",
    "\n",
    "欠測が起こっているデータを用いて解析を行う場合、単純に欠測が起こっているデータを削除して解析を行うと誤った結果を導いてしまう場合がある。例えば、\n",
    "\n",
    "* 入学試験の結果(X)と学年末試験の結果(Y)の相関\n",
    "* 新卒採用試験の結果(X)とその後のパフォーマンスの相関(Y)  \n",
    "\n",
    "など、XとYの関係を調べたいときに、単純な回帰分析  \n",
    "$$ Y = \\alpha + \\beta X $$\n",
    "を行って係数$\\beta$を見たり、相関係数  \n",
    "$$\\rho_{X, Y} = \\frac{cov(X, Y)}{\\sigma_{X} \\sigma_{Y}}$$  \n",
    "の値を用いて判断しようとすると、真の偏回帰係数や真の相関係数よりも値(の絶対値)が小さくなってしまう事がある。これを選抜効果という。  \n",
    "\n",
    "イメージ図\n",
    "\n",
    "選抜効果を考慮して相関関係を推定する方法に完全情報最尤推定法(FIML)というのがある。  \n",
    "FIMLの理論の説明や実装してみた系の記事は既にあるが、実際にどの程度使えるのかを検証した日本語の記事が見当たらなかったので、勉強を兼ねて実装した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 対象読者"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FIMLという名前を聞いたけど、何に使えるのかわからない人\n",
    "\n",
    "実装のコード無視してOK  \n",
    "FIMLのお気持ちと使い方を何となく理解して、  \n",
    "興味が湧けば(村山、2011)を読みに行くといいと思う。  \n",
    "http://koumurayama.com/koujapanese/missing_data.pdf\n",
    "\n",
    "* FIMLの理論は何となくわかったが、実装の仕方がわからない人\n",
    "\n",
    "お気持ちはサラッと読み流して、コード等を参考にしてください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この記事を読んだらわかる事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FIMLを使うと何が嬉しいのか\n",
    "* FIMLのざっくりとした理解\n",
    "* FIMLの実装例(Python + numpy + scipy)\n",
    "* FIMLを用いた仮説検証の例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この記事を読んでも(絶対に)わからない事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FIMLのちゃんとした理論  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIMLを使うと何が嬉しいのか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "観測データを用いて複数の変数の間の関係を調べたい時、観測データが発生した状況を考慮しないと、誤った解析結果を導いてしまう場合がある。  \n",
    "\n",
    "例えば、ある高校の入学試験の成績($X$)と高校入学後の学年末試験の成績($Y$)の相関を調べたいとする。入学試験の点数$x$が合格点$c$を下回った生徒は、仮に入学して学年末試験を受けていたら何点を取ったか、というデータ$y$が観測出来ない。なので、手元のデータは、受験したものの不合格になってしまった生徒の$Y$のデータが欠測している。  \n",
    "\n",
    "$Y$の欠測は完全にランダムに発生しているのではなく、\n",
    "* $X$が合格点を超えた(x >= c)→$Y$は観測\n",
    "* $X$が合格点を下回った(x < c)→$Y$は欠測  \n",
    "\n",
    "となっている。このように、全てが観測出来ている変数($X$)によって欠測するかどうかが決まるタイプの欠測をMARと言う。  \n",
    "\n",
    "$Y$が欠測するかどうかは完全にランダム(MCAR)であれば、$Y$が観測出来ていないデータを捨てて解析しても問題ないが、MARを仮定したデータに対して$Y$が欠測したデータを捨てて解析した場合、真の解析結果とは違う解析結果になってしまう。例えば、本当は$X$と$Y$に相関があるのに、$Y$が欠測した$X$のデータを捨てると相関が弱まったり見えなくなってしまう事がある(選抜効果)。  \n",
    "\n",
    "FIMLは、欠測がMARだった場合に、$Y$が観測出来たデータのみを用いて解析をするのではなく、$Y$が観測出来なかったデータも用いて解析する。  \n",
    "これにより、全てのデータが観測出来ていた場合の解析結果により近い解析を行うことが出来る。  \n",
    "\n",
    "上の例だと、合格者iのデータ$(x_i, y_i)$のみを用いて解析をするのではなく、不合格者jのデータ$(x_j,)$も用いて解析する事で、不合格者jが学年末試験を受けていた場合の解析結果により近い解析を行うことが出来る。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# で、FIMLは何なのか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理論的な説明は  \n",
    "欠損データ分析（missing data analysis）-完全情報最尤推定法と多重代入法-  \n",
    "http://koumurayama.com/koujapanese/missing_data.pdf  \n",
    "等に任せるとして、この記事ではFIMLのお気持ちを掴む。  \n",
    "ちゃんと理論を理解したければ、得体のしれないブログを読むよりちゃんとした教授が書いたPDFとか本を読んだ方が絶対理解が速い。  \n",
    "ただ、初手でガチな本を読んだりするのはとても骨が折れるし、興味を維持するのも大変なので、過度な数式は使わずに「お気持ち」をサクッと説明する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最尤推定のお気持ち"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIMLは最尤推定に基づいた考え方になっている。  \n",
    "最尤推定の説明は省くが、ざっくり言うと  \n",
    "* データ$x$が、パラメータ$\\sigma$によって決められた確率分布から発生している\n",
    "* 確率分布の式の形はわかっているが、パラメータ$\\sigma$の値がわからない\n",
    "* 手元に観測されたデータ$X = (x_1, x_2, ..., x_n)$がある\n",
    "\n",
    "としたときに、データ$X$を発生させる確率が最も高くなるパラメータ$\\sigma$の値を推定する方法。  \n",
    "\n",
    "未知のパラメータで形が決まる確率分布から手元のデータが発生する確率が、「最」も「尤」もらしくなる、高くなるようなパラメータを「推定」する。  \n",
    "\n",
    "具体的には、個々のデータの対数尤度を計算し、データ全体の合計値を最大化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIMLのお気持ち"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIMLのお気持ちを掴む。  \n",
    "* データ$(x, y)$が、複数のパラメータ$\\mathbf{\\sigma} = (\\sigma_1, \\sigma_2, ...)$によって決められたある同時確率分布から発生している\n",
    "* 確率分布の式の形はわかっているが、パラメータ$\\mathbf{\\sigma}$の値がわからない\n",
    "* パラメータの中には、**$y$と関係のあるパラメータ($y$の平均$\\mu_y$など)**と**関係のないパラメータ($x$の平均$\\mu_x$など)**がある\n",
    "* 手元に観測された$x$, $y$のデータがある。データの中には$(x, y)$が揃ったデータがあれば、$y$が欠測したデータ$(x,)$もある\n",
    "\n",
    "としたときに、  \n",
    "* $(x, y)$が揃っている→全てのパラメータの推定に使用  \n",
    "* $(x,)$しかない、$y$が欠測している→$y$と関係のないパラメータの推定に使用\n",
    "\n",
    "する事で、手元のデータ全てを用いてパラメータを推定する方法。  \n",
    "\n",
    "基本的には最尤推定と一緒。  \n",
    "全てが観測されたデータは通常の対数尤度、一部の変数が欠測した場合は欠測してない変数のみに関する確率分布を元の確率分布から抜き出してデータの対数尤度を計算し、データ全体の対数尤度の合計値が最大化するようなパラメータを探索する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIMLを実際に使ってみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装コードは、以下を参考にしました。  \n",
    "kamadak / fiml-py  \n",
    "https://github.com/kamadak/fiml-py/blob/master/fiml.py  \n",
    "Python+NumpyでFIML（完全情報最尤推定法）試してたら色々勘違いをしていたっぽい話  \n",
    "https://ensekitt.hatenablog.com/entry/2018/03/09/200000#fn-d1cba4eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "np.random.seed(1234)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題設定\n",
    "[(村山, 2011)](http://koumurayama.com/koujapanese/missing_data.pdf)に載っている例に従って、以下のような問題を設定する。  \n",
    "10人の被験者に対して、動機付け、IQテスト、適性検査を行なった。ただし、IQが100以下の被験者は適性検査を受けないとする、つまりIQが100以下の被験者の適性検査の結果は観測されない。\n",
    "ただし、\"test_full\"は欠測が無かった場合の結果で、実際の解析でこの列を用いる事は出来ない。\n",
    "\n",
    "## ゴール\n",
    "IQと適性検査の間に相関があるのかを明らかにしたい。\n",
    "\n",
    "## 確認したい事\n",
    "* FIMLを用いて計算したIQテストと適性検査の相関係数$\\rho$が、欠測データを捨てた場合と比べて、どれだけ真の値に近づけるか\n",
    "* FIMLを用いて計算した適性検査の標本平均が、欠測データを捨てた場合と比べて、どれだけ真の値に近づけるか\n",
    "* (村山, 2011)と同じ解析結果になっているか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データが発生する構造を定義する。  \n",
    "今回は、3変数(motivation, IQ, test)の確率変数ベクトル$\\mathbf{x}$が平均$\\mathbf{\\mu}$, 分散$\\Sigma$の多変量正規分布から発生するとする。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"motivation\": [3,4,5,2,5,3,2,6,3,3],\n",
    "    \"IQ\": [83,85,95,96,103,104,109,112,115,116],\n",
    "    \"test_obs\": [np.nan, np.nan, np.nan, np.nan,128, 102, 111,113,117,133],\n",
    "    \"test_full\": [93,99,98,103,128,102,111,113,117,133]\n",
    "})\n",
    "df = df[[\"motivation\", \"IQ\", \"test_obs\", \"test_full\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   motivation   IQ  test_obs  test_full\n",
      "0           3   83       NaN         93\n",
      "1           4   85       NaN         99\n",
      "2           5   95       NaN         98\n",
      "3           2   96       NaN        103\n",
      "4           5  103     128.0        128\n",
      "5           3  104     102.0        102\n",
      "6           2  109     111.0        111\n",
      "7           6  112     113.0        113\n",
      "8           3  115     117.0        117\n",
      "9           3  116     133.0        133\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上から4人分の\"test_obs\"が欠測している。  \n",
    "欠測は、\"IQ\"の値が100を下回るかどうかで決まっていることがわかる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "motivation      3.600000\n",
       "IQ            101.800000\n",
       "test_obs      117.333333\n",
       "test_full     109.700000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"test_obs\"と\"test_full\"で平均値が異なるのがわかる。  \n",
    "(村山, 2011)と\"test_full\"の平均値が違うけど、多分こちらが正しい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの標準化  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "地味に一番重要なところ。  \n",
    "値が大きいと計算途中でどんどん誤差が膨らんでいくので、予め標準化しておく。  \n",
    "標準化しないと(村山, 2011)の値を復元出来なかった。  \n",
    "相関係数の値はそのままで良いが、平均は最後に元の値に戻す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"motivation\", \"IQ\", \"test_obs\"]].values\n",
    "mean_array = np.nanmean(data, axis=0)\n",
    "std_array = np.nanstd(data, axis=0)\n",
    "# nanを無視して要素ごとに計算が出来る、便利\n",
    "data_std = (data - mean_array) / std_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 関数にデータを投入する準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "size, dim = data_std.shape\n",
    "\n",
    "mis_num = np.isnan(data_std).sum()\n",
    "mis_array = data_std[:mis_num, :-1]\n",
    "obs_array = data_std[mis_num:, :]\n",
    "\n",
    "obs_col = np.array([True for _ in range(dim)])\n",
    "mis_col = np.array([True for _ in range(dim-1)] + [False])\n",
    "\n",
    "data_blocks = [(mis_col, mis_array), (obs_col, obs_array)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIMLの実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIMLは、観測データの尤度を最大化するパラメータを求める。  \n",
    "観測データの対数尤度関数と初期パラメータを定義し、scipy.optimizeで最適化問題を解く。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean0 = np.zeros(dim)\n",
    "cov0 = np.eye(dim)\n",
    "params0 = np.empty(dim + (dim * (dim+1)) // 2)\n",
    "params0[:dim] = mean0\n",
    "params0[dim:] = cov0[np.tril_indices(dim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 観測データ全体の対数尤度関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _obj_func(params, dim, data_blocks):\n",
    "    mean = params[0:dim] # paramsから平均に関するパラメータをslice\n",
    "    cov = np.empty((dim, dim)) # 分散共分散行列を定義\n",
    "    ii, jj = np.tril_indices(dim) # 下三角行列の0じゃない部分を取り出す\n",
    "    cov[ii, jj] = params[dim:] # 分散共分散行列は対称行列なので、\n",
    "    cov[jj, ii] = params[dim:] # 下三角と上三角に同じ分散共分散を代入する\n",
    "    \n",
    "    # 分散共分散行列が対称な半正定値行列になっているかを判別する例外処理\n",
    "    # 半正定値行列じゃないと分散共分散行列に使えない\n",
    "    if (np.linalg.eigvalsh(cov) < 0).any():\n",
    "        return np.inf\n",
    "    objval = 0.0 # 対数尤度をここに加算していく \n",
    "    for obs, obs_data in data_blocks:\n",
    "        obs_mean = mean[obs]\n",
    "        obs_cov = cov[obs][:, obs]\n",
    "        objval += _log_likelihood_composed(obs_data, obs_mean, obs_cov)\n",
    "    return -objval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_likelihood_composed(x, mean, cov):\n",
    "    # 多変量正規分布の対数尤度を書き下した関数\n",
    "    # scipy.stats.multivariate_normal.logpdfを使っても良いが、こちらの方が2倍速かった\n",
    "    xshift = x - mean\n",
    "    size = x.shape[0]\n",
    "    t1 = x.shape[-1] * np.log(2*np.pi) # t1は無くても問題なさそう\n",
    "    sign, logdet = np.linalg.slogdet(cov)\n",
    "    t2 = logdet\n",
    "    t3 = -0.5 * xshift.dot(np.linalg.inv(cov)) * xshift\n",
    "    return (-0.5 * size * t1) + (-0.5 * size * t2) + t3.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 36.341192029461155\n",
      "            Iterations: 15\n",
      "            Function evaluations: 177\n",
      "            Gradient evaluations: 15\n"
     ]
    }
   ],
   "source": [
    "result = optimize.fmin_slsqp(\n",
    "    _obj_func, params0, args=(dim, data_blocks), disp=True, iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = result[0:dim]\n",
    "cov = np.empty((dim, dim))\n",
    "ii, jj = np.tril_indices(dim)\n",
    "cov[ii, jj] = result[dim:]\n",
    "cov[jj, ii] = result[dim:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQと適性検査の相関係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(i, j, cov):\n",
    "    return cov[i, j] / (cov[i,i]**(1/2) * cov[j,j]**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIMLで欠測データを考慮した相関係数: 0.669\n",
      "真の相関係数: 0.773\n",
      "欠測データを考慮しない相関係数: 0.342\n"
     ]
    }
   ],
   "source": [
    "# FIMLを用いて欠測を考慮した場合\n",
    "print(\"FIMLで欠測データを考慮した相関係数: {:.3f}\".format(corr(1, 2, cov)))\n",
    "\n",
    "# 欠測無しの適性検査から相関係数を計算した場合\n",
    "data_full = df[[\"motivation\", \"IQ\", \"test_full\"]].values\n",
    "print(\"真の相関係数: {:.3f}\".format(np.corrcoef(data_full.T)[1,2]))\n",
    "\n",
    "# 観測データのみから相関係数を計算した場合\n",
    "data_obs = df.loc[mis_num:, [\"motivation\", \"IQ\", \"test_obs\"]].values\n",
    "print(\"欠測データを考慮しない相関係数: {:.3f}\".format(np.corrcoef(data_obs.T)[1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠測データを考慮しないで相関係数を計算した結果は0.342となり、真の相関係数0.773と比べて相関がかなり過小評価されてしまっている。  \n",
    "\n",
    "一方で、FIMLを用いて相関係数を計算した結果は0.669となり、真値0.773に近いことがわかる。\n",
    "\n",
    "一般的に、FIMLを用いた相関係数が0.669は正の相関があると言えるが、欠測を考慮しないで計算した相関係数0.342では、正の相関は弱いとしか言えない。  \n",
    "\n",
    "観測データが発生した状況を考慮し、場合によっては欠測が起こっている前提でモデルを構築しなければ、誤った判断を下す危険性がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.60005085 101.79934184 110.91685252]\n"
     ]
    }
   ],
   "source": [
    "# 標準化したmeanを元に戻す\n",
    "print(mean * std_array + mean_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解値: [動機付け, IQ, 適性検査(欠測無し)] = [101.8, 117.3, 111,7]  \n",
    "(村山, 2017)の予測値: [3.6, 101.8, 110.9]  \n",
    "適性検査(欠測あり)の平均値は117.3  \n",
    "\n",
    "FIMLを用いることで、元々欠測の起こっていない「動機づけ」や「IQ」が真値と一致するのはもちろん、欠測が起きている変数「適性検査」の平均値も110.9となり、真の平均値111.7に近い値となった。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 完全情報最尤推定法は、MAR仮定の欠測データに対し、欠測によるバイアスを取り除いた推定が出来る\n",
    "* 仮想データに対して、欠測を考慮しない推定法に比べたFIMLの有用性を確認した"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 次やれたらやりたいこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stanでベイズ推定したり、多分切断分布仮定しても同じような事出来そう  \n",
    "データのスケールにどこまで対応出来るかも気になる  \n",
    "多くのデータは多分Xがバイナリ(アプリをダウンロードしたか/してないか)だから、それらの対応策も勉強したい  \n",
    "多重代入法とかで埋めた場合との比較検証も面白そう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.97102646e-05, -5.87826681e-05, -6.14900883e-01])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
