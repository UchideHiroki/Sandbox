{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self Attention付きのLSTMを用いて、文章を分類したり予測するモデルを作る。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感情分析はよくやられているので、今回はSelf attention付きのLSTMを用いた文章の分類モデルを作ります。  \n",
    "他クラス分類は未知のクラスに脆弱だったり、複数クラスに属する可能性(タグ付けなど)を考慮出来ません。  \n",
    "そこで、今回はSelf attentionを用いて文章があるクラスに属するかどうかを調べる2値分類モデルを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゴール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未知のレビューに対して、6クラスそれぞれに属するかどうかを判定するモデルを作成する "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確認すべき事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各クラスに対して、以下を確認する  \n",
    "\n",
    "2値分類のモデルの作成→回帰[0, 1]で解くか、分類[0, 1] or [1, 0]で解くか  \n",
    "↓  \n",
    "結局やってることは変わらない、Binary Cross Entropyを用いて解く、出力はスカラー(1になる確率)  \n",
    "他クラス分類であれば$[p_1, p_2, p_3]$とベクトルを渡すが、2クラスなら$[p_1, p_2]$を渡すか、$p$を渡すかの違い  \n",
    "$p$を渡せば、自動的に$p_2$が求まる  \n",
    "ただ、pytorchの関数上では使用する関数が異なるので注意、torch.nnなのかtorch.nn.functionalなのか  \n",
    "nnとfunctionalの違いはしんどいので深追いしない  \n",
    "\n",
    "precision, recallの確認  \n",
    "評価手法はまだ未確定、要検討  \n",
    "通常のLSTMと、self-attention付きLSTMで性能比較→全部やらなくても、単一カテゴリでいいか  \n",
    "どの単語にAttentionしているのかを可視化  \n",
    "\n",
    "未知のレビューに対する分類\n",
    "他クラス分類(1of n)にしたい場合は、所属する確率が最大になるモデルを採択  \n",
    "タグ付け(k of n)にしたい場合は、出力の確率が0.5より大きいかで判断する  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの準備  \n",
    "カテゴリ毎のデータのダウンロード  \n",
    "trainとtest、その他にtextを切り出し  \n",
    "カテゴリ毎に保存  \n",
    "カテゴリ名を指定すると、ダウンサンプリングされたtrain_data, test_dataが返ってくる関数を作成  \n",
    "\n",
    "#### 全ての語彙の格納  \n",
    "全カテゴリのデータの語彙を格納したVocabを作成  \n",
    "Vocabは単語⇄インデックスのやり取りや、語彙数の管理を行う  \n",
    "オンライン学習(未知の単語への対応)はまだよくわかっていない  \n",
    "格納したvocabを保存しておく  \n",
    "\n",
    "#### DataIteratorの作成  \n",
    "train_data, test_dataを元にインスタンスを作成し、forループを回すと自動的にバッチを作成するclassを定義  \n",
    "バッチは全ての単語がインデックスに変換されており、バッチ内で一番長い文の長さに合わせて0埋めされている  \n",
    "\n",
    "#### モデルの作成  \n",
    "入力: 文章  \n",
    "出力: スカラー(所属確率)  \n",
    "となるモデルを作成する  \n",
    "通常のLSTMと、self attention付きのLSTM2パターンを作成する  \n",
    "\n",
    "#### 学習に必要な関数の定義と学習の実行  \n",
    "損失関数や学習関数、パラメータの設定  \n",
    "同一iteration内で行う処理をまとめた関数の設定  \n",
    "KFoldでデータを5分割して、1つをvalidationとして使用(つまり、1回のiterationで5回学習を行う)  \n",
    "ラベルの偏りが出ないように、ラベルの値毎に層別で分割を行う  \n",
    "同一iterationのvalid_lossは、KFoldした値の平均値を採用\n",
    "learning_curveの監視の仕組みを整備  \n",
    "early stopping  \n",
    "学習  \n",
    "\n",
    "#### 学習済みモデルの保存  \n",
    "モデルとモデルのパラメータを保存  \n",
    "\n",
    "#### モデルの性能評価  \n",
    "test_dataに対して混同行列とF1-scoreを計算して見せる  \n",
    "カテゴリ毎の分類で、いい感じの分類器と微妙な分類器を見せる  \n",
    "上手くいったパターン、上手くいかなかったパターンの文章をself-attention付きで確認する  \n",
    "self-attentionありのモデルと無しのモデルでF1-scoreがどれだけ変わるのかをまとめる  \n",
    "\n",
    "#### 結論  \n",
    "まとめる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebookを分割した方が良さそう  \n",
    "このノートブックは、データの整形、前処理を行う  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb953cfa4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon review dataを使います。  \n",
    "URL: http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは、Amazonの様々なカテゴリの商品に対するレビューが集まったデータセットです。  \n",
    "20万レビューを超えているカテゴリを6つ選び、それぞれ10万サンプルをtrain, 残りの10万サンプルをtestに使用します。  \n",
    "それぞれのカテゴリのtrain_dataを合算し、それぞれのカテゴリに対して、そのカテゴリに属しているのかどうかを求める2値分類モデルを作成します。  \n",
    "\n",
    "Video Games  \n",
    "Home and Kitchen  \n",
    "Apps for Android  \n",
    "Health and Beauty  \n",
    "Clothing, Shoes and Jewelry  \n",
    "Sports and Outdoors  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataは、そのカテゴリに属しているデータが10万件、属していないデータが50万件とアンバランスなデータになっています。  \n",
    "そこで、ダウンサンプリングを行い、そのカテゴリに属していないデータを10万件に減らします。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カテゴリ毎に、データ数 × 1のDFを作成する  \n",
    "カテゴリ: aa, csj, hpc, hk, so, vg  \n",
    "train: 10万行, test: 10万行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vg ../input/reviews_Video_Games_5.json\n",
      "(1000,) (1000,)\n",
      "hk ../input/reviews_Home_and_Kitchen_5.json\n",
      "(1000,) (1000,)\n",
      "so ../input/reviews_Sports_and_Outdoors_5.json\n",
      "(1000,) (1000,)\n",
      "csj ../input/reviews_Clothing_Shoes_and_Jewelry_5.json\n",
      "(1000,) (1000,)\n",
      "hpc ../input/reviews_Health_and_Personal_Care_5.json\n",
      "(1000,) (1000,)\n",
      "aa ../input/reviews_Apps_for_Android_5.json\n",
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "p = pathlib.Path(\".\").glob(\"../input/reviews_*.json\")\n",
    "for category, path in zip([\"vg\", \"hk\", \"so\", \"csj\", \"hpc\", \"aa\"], p):\n",
    "    data = pd.read_json(path, lines=True)\n",
    "    data = data[\"reviewText\"].str.lower().str.replace(\"[^\\sa-z'-]\", \"\").str.replace(r\"\\s{2,}\", r\" \")\n",
    "    data = data[~data.isin([\"\", \" \"])].reset_index(drop=True)\n",
    "    idx = np.arange(data.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:1000]\n",
    "    test_idx = idx[1000:2000]\n",
    "    train_s = data.loc[train_idx]\n",
    "    test_s = data.loc[test_idx]\n",
    "    train_s.to_csv('../preprocessed/{}_train.csv'.format(category), index=False, encoding=\"utf-8\")\n",
    "    test_s.to_csv('../preprocessed/{}_test.csv'.format(category), index=False, encoding=\"utf-8\")\n",
    "    print(category, path)\n",
    "    print(train_s.shape, test_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviewTextに空白のレビューがあるせいでCSVで吐き出したレビューと吐き出す前で行数が一致していない  \n",
    "予め空白がある行を削除する  \n",
    "要らない文字を消す(\"\"にreplaceする)→余分な空白を消す(\" \"にreplaceする)→[\"\", \" \"]に一致する行を消去する  \n",
    "の手順を踏む"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単語をintに変換するための辞書作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNモデルにデータを投入するためには、単語(str)を数字(int)に変換する必要があります。  \n",
    "そのため、train_data, test_dataの単語をintに変換するためのクラスを作ります。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorchのチュートリアルが参考になります。  \n",
    "URL :https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = defaultdict(int)\n",
    "        self.word2count = defaultdict(int)\n",
    "        self.index2word = defaultdict(str)\n",
    "        self.n_words = 0\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 0\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 476 ms, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for path in glob.glob('../preprocessed/*.csv'):\n",
    "    series = pd.read_csv(path, header=None, dtype={0: str}, encoding='utf-8').dropna(axis=0)[0]\n",
    "    for sentence in series:\n",
    "        vocab.add_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaultdictは未知のkeyに対応するvalueを要求すると、defaultのvalueを作成してしまう\n",
    "# 後々のバグを防ぐため、通常のdictに変えてロックする\n",
    "vocab.word2index = dict(vocab.word2index)\n",
    "vocab.index2word = dict(vocab.index2word)\n",
    "vocab.word2count = dict(vocab.word2count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ジェネレータの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カッコいいので、ループするとバッチを返すジェネレータを作ります。  \n",
    "ジェネレータはイテレータを作成する関数のことをさします。  \n",
    "各バッチは、(レビュアー数, 最大文章長さ)となり、文章の長さがバッチ内最大の長さに足りない部分はゼロ埋めされます。  \n",
    "ジェネレータを以下のようにクラスで定義すると、何度でも使用する事が出来ます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: https://www.lifewithpython.com/2015/11/python-create-iterator-protocol-class.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチを作成する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_padded_array(reviews, vocab=vocab):  \n",
    "    review_list = list()\n",
    "    len_list = list()\n",
    "    for r in reviews:\n",
    "        review_indexes = [vocab.word2index[w] for w in r.split()]\n",
    "        review_list.append(review_indexes)\n",
    "        len_list.append(len(review_indexes))\n",
    "    \n",
    "    len_array = np.sort(len_list)[::-1].copy() # torch.Tensorは配列逆にしているとエラーを起こすので、コピーする\n",
    "    idxes = np.argsort(len_list)[::-1].copy()\n",
    "    text_array = np.zeros((len(review_list), max(len_list)), dtype=int)\n",
    "    for i, idx in enumerate(idxes):\n",
    "        text_array[i, :len(review_list[idx])] = review_list[idx]\n",
    "    return text_array, len_array, idxes + reviews.index[0] # idxesは0スタートなので、入力reviewsのindexと一致するように調整する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチを適宜作成する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BatchIterator(object):\n",
    "    def __init__(self, df, batch_len):\n",
    "        self.df = df\n",
    "        self.batch_len = batch_len\n",
    "        self.n_data = df.shape[0]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        df = self.df.sample(frac=1).reset_index(drop=True) # DFをシャッフルする\n",
    "        for b_idx in range(0, self.df.shape[0], self.batch_len):\n",
    "            text_batch = df.loc[b_idx:b_idx+self.batch_len-1, \"text\"]\n",
    "            target_batch = df.loc[b_idx:b_idx+self.batch_len-1, \"label\"]\n",
    "            \n",
    "            text_array, len_array, idxes = make_padded_array(text_batch)\n",
    "            # negative stride(降順に並び替え)するとtorch.LongTensor()が使えない、copy()して新しいメモリにarrayを作り変える\n",
    "            target_array = target_batch[idxes].values # batch内で順番を並び替えているので、targetもそれに合わせる\n",
    "            \n",
    "            \n",
    "            text_tensor = torch.LongTensor(text_array).to(device)\n",
    "            lengths_tensor = torch.LongTensor(len_array).to(device)\n",
    "            target_tensor = torch.LongTensor(target_array).to(device)\n",
    "            \n",
    "            yield text_tensor, lengths_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_iterator = BatchIterator(tmp, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 76]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 43]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 580]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 165]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 115]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 81]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 80]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 93]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 86]) torch.Size([10]) torch.Size([10])\n",
      "torch.Size([10, 40]) torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for te, l, ta in text_iterator:\n",
    "    print(te.shape, l.shape, ta.shape)\n",
    "    cnt += 1\n",
    "    if cnt >= 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
