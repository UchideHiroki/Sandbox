{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "np.random.seed(1234)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc8c8d05470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = defaultdict(int)\n",
    "        self.word2count = defaultdict(int)\n",
    "        self.index2word = defaultdict(str)\n",
    "        self.n_words = 0\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 0\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 672 ms, sys: 16 ms, total: 688 ms\n",
      "Wall time: 687 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = Vocab()\n",
    "\n",
    "for path in glob.glob('../preprocessed/*.csv'):\n",
    "    series = pd.read_csv(path, header=None, dtype={0: str}, encoding='utf-8').dropna(axis=0)[0]\n",
    "    for sentence in series:\n",
    "        vocab.add_sentence(sentence)\n",
    "\n",
    "# defaultdictは未知のkeyに対応するvalueを要求すると、defaultのvalueを作成してしまう\n",
    "# 後々のバグを防ぐため、通常のdictに変えてロックする\n",
    "vocab.word2index = dict(vocab.word2index)\n",
    "vocab.index2word = dict(vocab.index2word)\n",
    "vocab.word2count = dict(vocab.word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_padded_array(reviews, vocab=vocab):  \n",
    "    review_list = list()\n",
    "    len_list = list()\n",
    "    for r in reviews:\n",
    "        r = str(r)\n",
    "        review_indexes = [vocab.word2index[w] for w in r.split()]\n",
    "        review_list.append(review_indexes)\n",
    "        len_list.append(len(review_indexes))\n",
    "    \n",
    "    len_array = np.sort(len_list)[::-1].copy() # torch.Tensorは配列逆にしているとエラーを起こすので、コピーする\n",
    "    idxes = np.argsort(len_list)[::-1].copy()\n",
    "    text_array = np.zeros((len(review_list), max(len_list)), dtype=int)\n",
    "    for i, idx in enumerate(idxes):\n",
    "        text_array[i, :len(review_list[idx])] = review_list[idx]\n",
    "    return text_array, len_array, idxes + reviews.index[0] # idxesは0スタートなので、入力reviewsのindexと一致するように調整する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BatchIterator(object):\n",
    "    def __init__(self, df, batch_len):\n",
    "        self.df = df\n",
    "        self.batch_len = batch_len\n",
    "        self.n_batch = df.shape[0] // batch_len + 1\n",
    "        self.n_data = df.shape[0]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        df = self.df.sample(frac=1).reset_index(drop=True) # DFをシャッフルする\n",
    "        for b_idx in range(0, self.df.shape[0], self.batch_len):\n",
    "            text_batch = df.loc[b_idx:b_idx+self.batch_len-1, \"text\"]\n",
    "            target_batch = df.loc[b_idx:b_idx+self.batch_len-1, \"label\"]\n",
    "            \n",
    "            text_array, len_array, idxes = make_padded_array(text_batch)\n",
    "            target_array = target_batch[idxes].values\n",
    "            \n",
    "            text_tensor = torch.LongTensor(text_array).to(device)\n",
    "            lengths_tensor = torch.LongTensor(len_array).to(device)\n",
    "            target_tensor = torch.LongTensor(target_array).to(device)\n",
    "            \n",
    "            yield text_tensor, lengths_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多値分類のデータセットを作る  \n",
    "クラスを0~5に降る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_train = pd.read_csv('../preprocessed/vg_train.csv', header=None, encoding='utf-8')\n",
    "hk_train = pd.read_csv('../preprocessed/hk_train.csv', header=None, encoding='utf-8')\n",
    "so_train = pd.read_csv('../preprocessed/so_train.csv', header=None, encoding='utf-8')\n",
    "csj_train = pd.read_csv('../preprocessed/csj_train.csv', header=None, encoding='utf-8')\n",
    "hpc_train = pd.read_csv('../preprocessed/hpc_train.csv', header=None, encoding='utf-8')\n",
    "aa_train = pd.read_csv('../preprocessed/aa_train.csv', header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([vg_train, hk_train, so_train, csj_train, hpc_train, aa_train], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1000\n",
    "train_data['label'] = pd.Series([0]*i+[1]*i+[2]*i+[3]*i+[4]*i+[5]*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_test = pd.read_csv('../preprocessed/vg_test.csv', header=None, encoding='utf-8')\n",
    "hk_test = pd.read_csv('../preprocessed/hk_test.csv', header=None, encoding='utf-8')\n",
    "so_test = pd.read_csv('../preprocessed/so_test.csv', header=None, encoding='utf-8')\n",
    "csj_test = pd.read_csv('../preprocessed/csj_test.csv', header=None, encoding='utf-8')\n",
    "hpc_test = pd.read_csv('../preprocessed/hpc_test.csv', header=None, encoding='utf-8')\n",
    "aa_test = pd.read_csv('../preprocessed/aa_test.csv', header=None, encoding='utf-8')\n",
    "\n",
    "test_data = pd.concat([vg_test, hk_test, so_test, csj_test, hpc_test, aa_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "i = 1000\n",
    "test_data['label'] = pd.Series([0]*i+[1]*i+[2]*i+[3]*i+[4]*i+[5]*i)\n",
    "\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "test_data.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 880 ms, sys: 0 ns, total: 880 ms\n",
      "Wall time: 879 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_iterator = BatchIterator(train_data, 10)\n",
    "cnt = 0\n",
    "for te, l, ta in text_iterator:\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的にここに準拠  \n",
    "https://qiita.com/itok_msi/items/ad95425b6773985ef959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding(146467→100)  \n",
    "LSTM(100→32)  \n",
    "Attention(24)  \n",
    "MLP(32→1)  \n",
    "\n",
    "Target(2), neg→[1,0], pos→[0,1]  \n",
    "\n",
    "損失関数に binary_cross_entropy_with_logits を噛ませるので、モデルの出力を[0,1]に制限しなくても良い(sigmoidとlossを別にするより学習が安定する)  \n",
    "予測時には出力層にsigmoid関数を噛ませる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention有りのbiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnClassifer(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, v_size, n_class=2, bidirectional=True,\n",
    "                 batch_first=True):\n",
    "        super(LSTMClassifer, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.bi = 2 if bidirectional else 1\n",
    "        self.emb = nn.Embedding(v_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first, \n",
    "                            bidirectional = bidirectional)\n",
    "        \n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(h_dim, 24),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(24, 1)\n",
    "        )\n",
    "        \n",
    "        self.affine = nn.Linear(self.h_dim, n_class)\n",
    "        \n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = torch.zeros(self.bi, b_size, self.h_dim, device=device)\n",
    "        return (h0, h0) # LSTMはhiddenとcell2つの隠れ層が必要\n",
    "    \n",
    "    def forward(self, sentences, lengths):\n",
    "        batch_len = sentences.shape[0]\n",
    "        hidden, cell = self.init_hidden(batch_len)\n",
    "        embed = self.emb(sentences)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embed, lengths, batch_first=True)\n",
    "        output, hidden = self.lstm(packed_input, (hidden, cell))\n",
    "        output = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[0] # (b, s, h)\n",
    "        output = output[:, :, :self.h_dim] + output[:, :, self.h_dim:] # 正方向の隠れ層と逆方向の隠れ層を加算\n",
    "        \n",
    "        # Attention\n",
    "        attn = self.attn(output.view(-1, self.h_dim)) # (b,s,h)→(b*s,h)→(b*s,1)\n",
    "        attn = F.softmax(attn.view(batch_len, -1), dim=1).unsqueeze(2) # (b*s,1)→(b,s)→(b,s,1)\n",
    "        \n",
    "        output = (output * attn).sum(dim=1) # (b, s, h)→(b, h)\n",
    "        output = self.affine(output) # (b,h)→(b,c)\n",
    "        output = F.log_softmax(output, dim=1) # (b, c), 各データが各クラスに属した場合の対数尤度を計算\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifer2(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, v_size, n_class=2, bidirectional=True,\n",
    "                 batch_first=True):\n",
    "        super(LSTMClassifer2, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.bi = 2 if bidirectional else 1\n",
    "        self.emb = nn.Embedding(v_size, emb_dim)\n",
    "        self.flstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first, \n",
    "                            bidirectional = False)\n",
    "        self.blstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first, \n",
    "                            bidirectional = False)\n",
    "        self.affine = nn.Linear(self.h_dim * self.bi, n_class)\n",
    "        \n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = torch.zeros(1, b_size, self.h_dim, device=device)\n",
    "        return (h0, h0) # LSTMはhiddenとcell2つの隠れ層が必要\n",
    "    \n",
    "    def forward(self, sentences, l):\n",
    "        hidden, cell = self.init_hidden(sentences.shape[0])\n",
    "        embed = self.emb(sentences)\n",
    "        f_l = []\n",
    "        b_l = []\n",
    "        \n",
    "        fout, fhidden = self.flstm(embed[:,0,:].unsqueeze(1), (hidden, cell))\n",
    "        bout, bhidden = self.blstm(embed[:,-1,:].unsqueeze(1), (hidden, cell))\n",
    "        f_l.append(fout)\n",
    "        b_l.append(b_out)\n",
    "        for i in range(sentences.shape[1]-1):\n",
    "            fout, fhidden = self.flstm(embed[:,i+1,:].unsqueeze(1), fhidden)\n",
    "            bout, bhidden = self.blstm(embed[:,-i-2,:].unsqueeze(1), bhidden)\n",
    "            f_l.append(fout)\n",
    "            g_l.append(gout)\n",
    "        \n",
    "        output = torch.cat((fout, bout), dim=2).squeeze(0)\n",
    "        \n",
    "        output = self.affine(output)\n",
    "        output = F.log_softmax(output, dim=1) # (b, n_class), 各データが各クラスに属した場合の対数尤度を計算\n",
    "        # logをとっても、最終的に選択されるカテゴリは対数尤度が最も大きい次元になる\n",
    "        # 損失関数は、F.nll_loss(negative loss likelihood loss)\n",
    "        # 入力するものは、各ラベルに所属する対数尤度(log_softmaxで計算)と正解ラベル\n",
    "        # 損失関数の中では、正解ラベルに所属する対数尤度の符号を反転させて足し合わせたものが加算されていく\n",
    "        # 式の形を見ればわかるが、cross entropyは正解ラベルに属するlog(sigmoid(x))のみを足し合わせている\n",
    "        # 従って、ネットワークのアウトプットをlog_softmax(今回は2カテゴリなのでlog_sigmoidとなる)とすれば良い\n",
    "        # 但し、log_softmaxはexp(-x)を計算しているので、xが極端に小さいとオーバーフローを起こすので注意\n",
    "        # とはいえlogとsoftmaxを別々で計算するよりは計算が安定する、、ここら辺は詳細不明\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習関数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, train_iter, optimizer, log_interval=100):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    all_ = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for idx, (x, x_l, y) in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output, attn = model(x, x_l)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        pred = output.data.max(dim=1)[1]\n",
    "        correct += pred.eq(y).sum().item() # 予測と実測の正答数を加算\n",
    "        all_ += len(y)\n",
    "     \n",
    "        if idx % log_interval == 0:\n",
    "            # バッチ毎の更新で十分にaccuracyが上がっていくので、そこの進捗を表示する\n",
    "            print('train epoch: {} [{}/{}], acc:{:.4f}, loss:{:.4f}'.format(\n",
    "                epoch, idx+1, train_iter.n_batch, correct/all_, loss))\n",
    "            correct = 0\n",
    "            all_ = 0\n",
    "    return epoch_loss / idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(epoch, test_iter, log_interval=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        epoch_loss = 0\n",
    "        for idx, (x, x_l, y) in enumerate(test_iter):\n",
    "            output, attn = model(x, x_l)\n",
    "            loss = F.nll_loss(output, y)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            pred = output.data.max(dim=1)[1]\n",
    "            correct += pred.eq(y).sum().item()\n",
    "            \n",
    "    if epoch % log_interval == 0:\n",
    "        print('test epoch: {}, acc:{:.4f}, loss:{:.4f}'.format(\n",
    "        epoch, correct/test_iter.n_data, epoch_loss))\n",
    "    return epoch_loss / idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(review):\n",
    "    review_idxes = [vocab.word2index[w] for w in str(review).split()]\n",
    "    review_tensor = torch.LongTensor(review_idxes).to(device).unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor([len(review_idxes)]).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out, attn = model(review_tensor, length_tensor)\n",
    "    \n",
    "    return out.max(dim=1)[1].item(), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifer(100, 32, vocab.n_words, n_class=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BatchIterator(train_data, batch_len=1)\n",
    "test_iter = BatchIterator(test_data, batch_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "patience = 5\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2) (6000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 0 [1/6001], acc:0.0000, loss:1.7187\n",
      "train epoch: 0 [1001/6001], acc:0.3340, loss:1.3989\n",
      "train epoch: 0 [2001/6001], acc:0.4890, loss:0.6746\n",
      "train epoch: 0 [3001/6001], acc:0.5640, loss:1.3322\n",
      "train epoch: 0 [4001/6001], acc:0.5970, loss:1.3639\n",
      "train epoch: 0 [5001/6001], acc:0.6370, loss:0.0262\n",
      "test epoch: 0, acc:0.6742, loss:5191.3131\n",
      "train epoch: 1 [1/6001], acc:1.0000, loss:0.4417\n",
      "train epoch: 1 [1001/6001], acc:0.7960, loss:0.0403\n",
      "train epoch: 1 [2001/6001], acc:0.7770, loss:1.6920\n",
      "train epoch: 1 [3001/6001], acc:0.7930, loss:2.6870\n",
      "train epoch: 1 [4001/6001], acc:0.7950, loss:0.0237\n",
      "train epoch: 1 [5001/6001], acc:0.7870, loss:0.0615\n",
      "test epoch: 1, acc:0.7143, loss:4667.1000\n",
      "train epoch: 2 [1/6001], acc:1.0000, loss:0.0230\n",
      "train epoch: 2 [1001/6001], acc:0.9220, loss:0.0107\n",
      "train epoch: 2 [2001/6001], acc:0.9120, loss:2.0058\n",
      "train epoch: 2 [3001/6001], acc:0.9050, loss:0.0073\n",
      "train epoch: 2 [4001/6001], acc:0.8990, loss:0.0001\n",
      "train epoch: 2 [5001/6001], acc:0.9050, loss:0.0087\n",
      "test epoch: 2, acc:0.7117, loss:5534.6711\n",
      "train epoch: 3 [1/6001], acc:1.0000, loss:0.0022\n",
      "train epoch: 3 [1001/6001], acc:0.9720, loss:0.0053\n",
      "train epoch: 3 [2001/6001], acc:0.9790, loss:0.0136\n",
      "train epoch: 3 [3001/6001], acc:0.9700, loss:0.0015\n",
      "train epoch: 3 [4001/6001], acc:0.9650, loss:0.0663\n",
      "train epoch: 3 [5001/6001], acc:0.9510, loss:0.0168\n",
      "test epoch: 3, acc:0.7275, loss:5812.7920\n",
      "train epoch: 4 [1/6001], acc:1.0000, loss:0.0279\n",
      "train epoch: 4 [1001/6001], acc:0.9970, loss:0.0080\n",
      "train epoch: 4 [2001/6001], acc:0.9930, loss:0.0026\n",
      "train epoch: 4 [3001/6001], acc:0.9860, loss:0.0001\n",
      "train epoch: 4 [4001/6001], acc:0.9850, loss:0.0808\n",
      "train epoch: 4 [5001/6001], acc:0.9780, loss:0.0264\n",
      "test epoch: 4, acc:0.7363, loss:6397.8151\n",
      "train epoch: 5 [1/6001], acc:1.0000, loss:0.0000\n",
      "train epoch: 5 [1001/6001], acc:0.9940, loss:0.0002\n",
      "train epoch: 5 [2001/6001], acc:0.9980, loss:0.0069\n",
      "train epoch: 5 [3001/6001], acc:0.9950, loss:0.0000\n",
      "train epoch: 5 [4001/6001], acc:0.9940, loss:0.0312\n",
      "train epoch: 5 [5001/6001], acc:0.9900, loss:0.0000\n",
      "test epoch: 5, acc:0.7293, loss:7705.8377\n",
      "train epoch: 6 [1/6001], acc:1.0000, loss:0.0002\n",
      "train epoch: 6 [1001/6001], acc:0.9970, loss:0.0004\n",
      "train epoch: 6 [2001/6001], acc:0.9990, loss:0.0045\n",
      "train epoch: 6 [3001/6001], acc:0.9970, loss:0.0000\n",
      "train epoch: 6 [4001/6001], acc:0.9970, loss:0.0001\n",
      "train epoch: 6 [5001/6001], acc:0.9990, loss:0.0059\n",
      "test epoch: 6, acc:0.7287, loss:8115.9433\n",
      "early stopping: epoch 6\n",
      "Done !\n",
      "CPU times: user 18min 59s, sys: 3min 2s, total: 22min 2s\n",
      "Wall time: 22min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_lc = []\n",
    "test_lc = []\n",
    "cnt = 0\n",
    "for epoch in range(n_epoch):\n",
    "    train_loss = train_model(epoch, train_iter, optimizer, log_interval=1000) # 学習が高速に進む+iter時間かかる→batch毎に進捗プリント\n",
    "    train_lc.append(train_loss)\n",
    "    \n",
    "    test_loss = test_model(epoch, test_iter, log_interval=1)\n",
    "    test_lc.append(test_loss)\n",
    "    \n",
    "    if epoch > 0:\n",
    "        if test_loss > min(test_lc[:-1]):\n",
    "            cnt += 1\n",
    "        else:\n",
    "            cnt = 0\n",
    "    \n",
    "    if cnt >= patience:\n",
    "            print('early stopping: epoch {}'.format(epoch))\n",
    "            break\n",
    "    \n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.843774317522648"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.7891019178164127, 2.843774317522648]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数60万(10万×6カテゴリ)だと学習が遅いし別にここまでデータ増やす必要もないかも  \n",
    "データ数6万(1万×6カテゴリ)に変えようかな"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../output/attn_params_600k.picke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  18,  136,   53, ...,    3,   23,   68],\n",
       "       [ 336,  386,  193, ...,    0,    0,    0],\n",
       "       [ 537, 1318, 1278, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1332,   20,  208, ...,    0,    0,    0],\n",
       "       [  18,  413,  118, ...,    0,    0,    0],\n",
       "       [  23, 2384,   28, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'solid'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index2word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index:0の単語と、0埋めした0が被っていそう  \n",
    "全体の精度には影響しないけど、気持ち悪いので最終的には取り除く"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
